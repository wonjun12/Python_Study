# 데이터 수집 기본 방법에 대해 배웠다.

## 1. 특정 검색 구간에서 사진 가져 오기 
```python 3
import requests # import
from bs4 import BeautifulSoup # import

search_url = "https://www.google.com/search?tbm=isch&q=" # 검색이 주되는 url
keyword = "맥주" # 검색할 쿼리
url = search_url + keyword

# get()으로 정보를 담는다.
response = requests.get(url)
# 받은 정보들의 text를 전부 받는다.
html = response.text
# 받은 정보들의 text를 객체화 시킨다.
soup = BeautifulSoup(html, "html.parser")

#print(soup.find('div')) # 맨 앞을 찾아서 반환한다.
#print(soup.find_all('div')) # find_all 전부 찾아서 list로 반환한다.

# img의 태그를 전부 가져와서 list로 변환한다.
imgs = soup.find_all('img')

import os #os import
# 만들거나 참고할 폴더의 이름
folder_name = 'imgs'
# 해당 폴더가 있다면 만들지 말고, 없다면 만든다.
if not os.path.exists(folder_name):
    os.mkdir(folder_name)

# 각각의 list마다 번호를 넣어서 사진에게 고유 키값을 가지게 하는 for문 작성
for dix, img in enumerate(imgs[1:]):
    file_name = f"img_{dix}.jpg" #파일 이름
    file_path = os.path.join(folder_name, file_name) # 경로 합쳐서 해당 경로에 만든다.
    img_response = requests.get(img['src']) # 해당 사진의 주소를 들고 온 후,
    img_data = img_response.content # 해당 사진의 값을 들고온다.
    with open(file_path, "wb") as f: # 사진의 값에는 Byte단위로 들어 있는 기에 Byte단위로 넣을 수 있게 wb를 사용
        f.write(img_data) # 파일에 작성한다.
    # 해당 for문이 끝나면 jpg 확장자에 해당 이미지의 레이아웃 값들이 들어가 정상적으로 이미지가 출력된다.
```

## 2. 뉴스에서 타이틀 제목과 연결된 내용들 저장하기.
```python 3
import requests # import
from bs4 import BeautifulSoup #import

import os # import
folder_name = "test_string_Crawling" # 폴더 이름 지정
file_name = 'headlines.txt' # 파일 이름 지정
if not os.path.exists(folder_name): # 폴더 여부 확인
    os.mkdir(folder_name) # 폴더 만듬
file_path = os.path.join(folder_name, file_name) # 해당 폴더와 파일로 경로 지정

headers = {"User-Agent":"Mozilla/5.0"} # 일반적인 접근이 불가능해 강제로 접속 할 수 있게끔 옵션을 넣는다.
url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=105' # 기본 바탕이 될 뉴스 URL
response = requests.get(url, headers = headers) # get() 사용해 정보 취합
html = response.text # response안에 text들만 모아 저장
soup = BeautifulSoup(html, "html.parser") # 저장된 text들이 객체로 만든다.
div = soup.body.find('div', attrs={'class': 'list_body'}) # 얻고 싶은 원하는 부분의 class가 맞는 div를 정보를 가져온다.
headlines = div.find_all('a', attrs={'class' : 'cluster_text_headline'}) 
# 얻고 싶은 원하는 것들의 class가 맞는 a 태그 들의 정보를 list의 정보를 가져온다.


for headline in headlines: # 가져온 list들을 반복문 돌림
    response = requests.get(headline['href'], headers = headers) # 해당 반복문에서 a 태그에 있는 link를 찾아 가져온다.
    html = response.text
    soup = BeautifulSoup(html, "html.parser") # 객체화 시킨 후
    div_texts = soup.body.find('div', attrs={'id' : 'dic_area'}).find_all(text=True) 
        # id가 dic_area인 div 박스를 찾아서 가져올 수 있는데,
        # 맨뒤 find_all(text=True)를 통해 <br> 태그도 같이 가져올 수 있도록 한다.

    # a는 경로에 자동으로 폴더를 만들 수 없다.
    # w는 경로에 자동으로 폴더를 만든다.    
    with open(file_path, 'a', encoding="UTF-8") as f:
        f.write(f"{headline.text.strip()}\n") # 제목을 넣고
        for div_text in div_texts: # 얻는 내용의 list값들을 돌려 넣는다.
            # <br>이 \n으로 자동을 변환되어 있기에 따로 안넣어도 된다.
            f.write(f"{div_text}")
        
        f.write(f"==========================================\n")
```